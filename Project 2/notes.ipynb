{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Miniproject 2\n",
    "Dragi Kamov and Aadil Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as la\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.loadtxt('data/mfeat-pix.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction \n",
    "Feature extraction is a used in machine learning to derive values (features) that are important from a dataset. It is used to alleviate the cure of dimensionality by reducing the dimensions of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of features \n",
    "- Hand-made features refer to properties derived from human insight on information that is in the images\n",
    "- K-means based features groups a collection of training data points related clusters $C_1,..., C_K$. Each cluster  $C_i$ is then represented by a codebook vector $c_i$, these codebook vectors are then used to compress data.\n",
    "- Principal Component Analysis (PCA) extracts features by converting a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables (principal components) using an orthogonal transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In our program, we will use PCA for feature extraction:\n",
    "### The steps in PCA algorithm follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- centering the data: mapping each vector $x_i \\to \\bar{x_i}$, where $\\bar{x_i} = x_i - \\bar{x}$\n",
    " - calculating the $\\mu_1, \\mu_2, ... \\mu_m$, where $m$ is the number of principle components\n",
    "   $\\mu_i$ is calculated via $SVD$ algorithm.\n",
    "   \n",
    "   1. We calculate the correlation matrix $R$.\n",
    "   2. Calculate the SVD via $$[U, \\Sigma, V^*] = SVD(R)$$, where\n",
    "   $$ R = U \\Sigma V^* $$\n",
    "   3. Extract the principle components $\\mu_1, \\mu_2, ... \\mu_m$, where $ m < n$, by taking the first $m$ columns of $U$. \n",
    " - Compression: we take each $\\mu_1, \\mu_2, \\mu_3 ... $ and dot product the already centered $\\bar{x_i}$ to obtain a vector $v \\in \\mathbb{R}^m$\n",
    " \\begin{align*}\n",
    " v = \\begin{bmatrix}\n",
    " \\mu_1 \\cdot \\bar{x_i} \\\\\n",
    "  \\mu_2 \\cdot \\bar{x_i} \\\\\n",
    "   \\mu_3 \\cdot \\bar{x_i} \\\\\n",
    "    \\mu_4 \\cdot \\bar{x_i} \\\\\n",
    "    ... \\\\\n",
    "    \\mu_m \\cdot \\bar{x_i}\n",
    " \\end{bmatrix}\n",
    " \\end{align*}\n",
    " - Decompression: we simply calculate\n",
    " $$ recovered = \\sum_{i = 1}^{m} v_i \\cdot \\mu_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we split the training data into training set $X$ and test set $Xtest$, and build its associated label one-hot encoding vector matrix $Y$ and $Ytest$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding bais\n",
    "\n",
    "We then create a fuction to add the bias term to the features. Linear regression will create a model based on offine function, which contains a bais term. Without it, we can only approximate the data using linear function, and it would lead to a very bad model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bais(X):\n",
    "    N, D = X.shape\n",
    "    Y = np.ones((N, D + 1))\n",
    "    Y[:,:-1] = X\n",
    "    return Y\n",
    "def square_norm(x):\n",
    "    return np.sum(np.power(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use One-hot encoding to generate class vector\n",
    "Since the dataset doesn't contain any kind of label, we need to generate a class vector for each label $\\{0, 1, 2, ... 10\\}$ as $v \\in \\mathbb{R}^{10}$. The following function will execute such a strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(digit):\n",
    "    rst = np.zeros(10)\n",
    "    rst[digit] = 1\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Implementation\n",
    "For a given dataset $data$ and a fixed number of feature $k$,\n",
    "our algorithm of linear regression proceed as the following:\n",
    "1. Performing a PCA algorithm to reduce the dimensions of $data$ from $\\mathbb{R}^{240}$ to $\\mathbb{R}^{k}$. Thus, we can view PCA algorithms as a function $PCA: \\mathbb{R}^{240} \\to \\mathbb{R}^{k}$\n",
    "2. Split the entire dataset after dimension reduction into training set features $X \\in \\mathbb{R}^{1000 \\times k}$ and test set features $X_{test} \\in \\mathbb{R}^{1000 \\times k}$\n",
    "3. Associate $X$ and $X_{test}$ with bias term, thus we have $X, X_{test} \\in \\mathbb{R}^{1000 \\times (K + 1)}$ \n",
    "4. Build the correct class vector for training set as $Y \\in \\mathbb{R}^{1000 \\times 10}$ and test set as $Y_{test} \\in \\mathbb{R}^{1000 \\times 10}$. After such operation, we obtained the complete training set as $(X, Y)$ and the test set as $(X_{test}, Y_{test})$\n",
    "5. Using the training set, compute the optimal weight matrix as\n",
    "$$ {W_{opt}}^\\top = (\\frac{1}{N} \\cdot X \\cdot X^\\top + \\alpha^2 \\cdot I_{nxn})^{-1} \\cdot \\frac{1}{N} \\cdot X \\cdot Y $$\n",
    "we can rewrite as\n",
    "$$ W_{opt} = ((\\frac{1}{N} \\cdot X \\cdot X^\\top + \\alpha^2 \\cdot I_{nxn})^{-1} \\cdot \\frac{1}{N} \\cdot X \\cdot Y)^\\top $$\n",
    "6. Calculate the error term.\n",
    "First, we make the prediction:\n",
    "$$ Y_{pred} = (W_{opt} \\cdot X)^\\top $$\n",
    "$$ Y_{test}pred = (W_{opt} \\cdot X_{test})^\\top $$\n",
    "Using the prediction, we calculate the corresponding error\n",
    "$$ MSE_{train} = \\frac{\\|Y - Ypred\\|^2}{1000} $$\n",
    "$$ MSE_{test} = \\frac{\\|Ytest - Y_{test}pred\\|^2}{1000} $$\n",
    "$$ MISS_{train} = \\frac{\\sum_{i = 1}^{1000} \\min(1, \\|\\arg\\max(Y_i) - \\arg\\max(Ypred_i)\\|)}{1000} $$\n",
    "$$ MISS_{test} = \\frac{\\sum_{i = 1}^{1000} \\min(1, \\|\\arg\\max({Y_{test}}_i) - \\arg\\max(Y_{test}pred_i)\\|)}{1000} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(data, k):\n",
    "    # do a linear regression on feature of number k\n",
    "    \n",
    "    #perform pca\n",
    "    pca = PCA(n_components=k)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    \n",
    "    # split the training set\n",
    "    X = np.ones((1000, k))\n",
    "    Xtest = np.ones((1000, k))\n",
    "    for i in range(10):\n",
    "        X[i*100:i*100 + 100][:] = data_pca[i*200:i*200 + 100][:]\n",
    "        Xtest[i*100:i*100 + 100][:] = data_pca[i*200 + 100:i*200 + 200][:]\n",
    "    \n",
    "    # add bias term\n",
    "    X = add_bais(X)\n",
    "    Xtest = add_bais(Xtest)\n",
    "    \n",
    "    # build the class\n",
    "    Y = np.zeros((1000, 10))\n",
    "    Ytest = np.zeros((1000, 10))\n",
    "    \n",
    "    for i in range(1000):\n",
    "        digit = i // 100\n",
    "        Y[i] = onehot_encode(digit).T #assign the one-hot encoding\n",
    "        Ytest[i] = onehot_encode(digit).T #same here\n",
    "        \n",
    "    # calculate the optimal weight\n",
    "    Wopt = np.matmul(np.matmul(la.inv(np.matmul(np.dot((1/1000), X), X.T)), X), Y).T\n",
    "    \n",
    "    #calculate the training error term\n",
    "    # first make the prediction\n",
    "    Ypred = np.matmul(Wopt,X).T\n",
    "    Ytestpred = np.matmul(Wopt, Xtest).T\n",
    "    \n",
    "    #calculate the error\n",
    "    mse_train = square_norm(Ypred - Y) / 1000.0\n",
    "    num_miss_train = 0\n",
    "    for i in range(1000):\n",
    "        if np.argmax(Ypred[i]) != np.argmax(Y[i]):\n",
    "            num_miss_train = num_miss_train + 1\n",
    "    miss_train = num_miss_train / 1000.0\n",
    "    \n",
    "    mse_test = square_norm(Ytestpred - Ytest) / 1000.0\n",
    "    num_miss_test = 0\n",
    "    for i in range(1000):\n",
    "        if np.argmax(Ytestpred[i]) != np.argmax(Ytest[i]):\n",
    "            num_miss_test = num_miss_test + 1\n",
    "    miss_test = num_miss_test / 1000.0\n",
    "    \n",
    "    return Wopt, mse_train, miss_train, mse_test, miss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f1a782ebe806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mse_train = {}, miss_train = {}, mse_test = {}, miss_test = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-318bf6694936>\u001b[0m in \u001b[0;36mlinear_regression\u001b[0;34m(data, k)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# calculate the optimal weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mWopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#calculate the training error term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "_, mse_train, miss_train, mse_test, miss_test = linear_regression(vectors, 40)\n",
    "print('mse_train = {}, miss_train = {}, mse_test = {}, miss_test = {}'.format(mse_train, miss_train, mse_test, miss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
